# main.py
"""Application principale Streamlit pour l'assistant IA avec support multi-fichiers"""

import streamlit as st
from file_handler import FileHandler
from ai_service import AIService
from ui_components import UIComponents
from config import PAGE_CONFIG, CUSTOM_CSS

def main():
    """Fonction principale de l'application"""
    
    # Configuration de la page
    st.set_page_config(**PAGE_CONFIG)
    
    # CSS personnalis√©
    st.markdown(CUSTOM_CSS, unsafe_allow_html=True)
    
    # Initialisation du session state
    UIComponents.initialize_session_state()
    
    # En-t√™te principal
    st.markdown('<div class="main-header"><h1>ü§ñ Assistant IA Multi-Documents</h1><p>Analysez vos documents avec l\'intelligence artificielle</p></div>', unsafe_allow_html=True)
    
    # Configuration de la barre lat√©rale
    selected_model, temperature, max_tokens = UIComponents.setup_sidebar()
    
    # Interface principale
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # Zone d'upload multiple
        uploaded_files = UIComponents.display_multi_file_upload()
        
        # Traitement des fichiers upload√©s
        if uploaded_files:
            UIComponents.process_uploaded_files(uploaded_files)
        
        # Affichage des fichiers en attente
        has_pending_files = UIComponents.display_pending_files()
        
        # Zone d'analyse globale
        if has_pending_files:
            analysis_question = UIComponents.display_analysis_input()
            
            if analysis_question:
                handle_multi_file_analysis(analysis_question, selected_model, temperature, max_tokens)
    
    with col2:
        # Upload de fichier simple (pour compatibilit√©)
        st.markdown("### üìÑ Upload simple")
        uploaded_file = UIComponents.display_file_upload()
        
        if uploaded_file:
            handle_single_file_analysis(uploaded_file, selected_model, temperature, max_tokens)
    
    # Affichage de l'historique des conversations
    st.markdown("---")
    st.markdown("### üí¨ Conversation")
    
    # Zone de chat
    UIComponents.display_chat_history()
    
    # Input pour le chat libre
    if prompt := st.chat_input("Posez une question..."):
        handle_chat_message(prompt, selected_model, temperature, max_tokens)

def handle_multi_file_analysis(question, selected_model, temperature, max_tokens):
    """G√®re l'analyse de plusieurs fichiers"""
    try:
        # Cr√©er le message utilisateur
        user_msg = UIComponents.create_multi_file_message(
            st.session_state["pending_files"], 
            question
        )
        st.session_state["messages"].append(user_msg)
        
        # Afficher le message utilisateur
        with st.chat_message("user"):
            st.markdown(f"üîç **Question:** {question}")
            st.markdown(f"üìÅ **Fichiers analys√©s:** {len(st.session_state['pending_files'])}")
        
        # Analyser avec l'IA
        with st.chat_message("assistant"):
            with st.spinner("üîç Analyse en cours..."):
                try:
                    # V√©rifier les avertissements
                    has_images = any(f['is_image'] for f in st.session_state["pending_files"])
                    warnings = AIService.validate_model_for_content(selected_model, has_images)
                    
                    if warnings:
                        UIComponents.display_warnings(warnings)
                    
                    # Lancer l'analyse
                    response = AIService.analyze_multiple_files(
                        st.session_state["pending_files"],
                        question,
                        selected_model,
                        temperature,
                        max_tokens
                    )
                    
                    st.markdown(response)
                    
                    # Ajouter la r√©ponse √† l'historique
                    st.session_state["messages"].append({
                        "role": "assistant",
                        "content": response
                    })
                    
                    # Vider les fichiers en attente apr√®s analyse
                    st.session_state["pending_files"] = []
                    
                except Exception as e:
                    error_msg = str(e)
                    UIComponents.display_error(error_msg, has_images, selected_model)
                    
                    # Ajouter l'erreur √† l'historique
                    st.session_state["messages"].append({
                        "role": "assistant",
                        "content": f"‚ùå Erreur: {error_msg}"
                    })
        
        # Forcer le rafra√Æchissement
        st.rerun()
        
    except Exception as e:
        st.error(f"Erreur lors de l'analyse: {e}")

def handle_single_file_analysis(uploaded_file, selected_model, temperature, max_tokens):
    """G√®re l'analyse d'un fichier unique"""
    try:
        # V√©rifier si le fichier a d√©j√† √©t√© trait√©
        file_hash = FileHandler.get_file_hash(uploaded_file)
        
        # V√©rifier dans l'historique si le fichier a d√©j√† √©t√© analys√©
        already_processed = any(
            msg.get("file_hash") == file_hash 
            for msg in st.session_state["messages"] 
            if msg["role"] == "user"
        )
        
        if not already_processed:
            # Extraire le contenu
            with st.spinner("üìñ Extraction du contenu..."):
                content = FileHandler.extract_text(uploaded_file)
            
            if content and not content.startswith("Erreur"):
                # Cr√©er le message utilisateur
                user_msg = UIComponents.create_user_message(uploaded_file, content, file_hash)
                st.session_state["messages"].append(user_msg)
                
                # Analyser avec l'IA
                with st.spinner("üß† Analyse en cours..."):
                    try:
                        response = AIService.analyze_file(
                            uploaded_file,
                            content,
                            selected_model,
                            temperature,
                            max_tokens
                        )
                        
                        # Ajouter la r√©ponse √† l'historique
                        st.session_state["messages"].append({
                            "role": "assistant",
                            "content": response
                        })
                        
                        st.rerun()
                        
                    except Exception as e:
                        error_msg = str(e)
                        is_image = FileHandler.is_image_file(uploaded_file)
                        UIComponents.display_error(error_msg, is_image, selected_model)
                        
                        # Ajouter l'erreur √† l'historique
                        st.session_state["messages"].append({
                            "role": "assistant",
                            "content": f"‚ùå Erreur: {error_msg}"
                        })
            else:
                st.error(f"Impossible d'extraire le contenu: {content}")
        else:
            st.info("üìÑ Ce fichier a d√©j√† √©t√© analys√© dans cette session")
            
    except Exception as e:
        st.error(f"Erreur lors du traitement du fichier: {e}")

def handle_chat_message(prompt, selected_model, temperature, max_tokens):
    """G√®re les messages de chat libre"""
    try:
        # Ajouter le message utilisateur
        st.session_state["messages"].append({
            "role": "user",
            "content": prompt
        })
        
        # Afficher le message utilisateur
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # G√©n√©rer la r√©ponse
        with st.chat_message("assistant"):
            with st.spinner("ü§î R√©flexion..."):
                try:
                    response = AIService.chat_with_ai(
                        st.session_state["messages"],
                        selected_model,
                        temperature,
                        max_tokens
                    )
                    
                    st.markdown(response)
                    
                    # Ajouter la r√©ponse √† l'historique
                    st.session_state["messages"].append({
                        "role": "assistant",
                        "content": response
                    })
                    
                except Exception as e:
                    error_msg = str(e)
                    UIComponents.display_error(error_msg, False, selected_model)
                    
                    # Ajouter l'erreur √† l'historique
                    st.session_state["messages"].append({
                        "role": "assistant",
                        "content": f"‚ùå Erreur: {error_msg}"
                    })
        
        st.rerun()
        
    except Exception as e:
        st.error(f"Erreur lors du chat: {e}")

def display_help_section():
    """Affiche la section d'aide"""
    with st.expander("‚ùì Aide et informations"):
        st.markdown("""
        ### üöÄ Comment utiliser l'assistant
        
        **1. Upload multiple de fichiers:**
        - S√©lectionnez plusieurs fichiers en une fois
        - Les fichiers sont mis en attente d'analyse
        - Posez une question globale pour analyser tous les fichiers
        
        **2. Upload simple:**
        - Upload d'un seul fichier pour une analyse imm√©diate
        - Compatible avec tous les formats support√©s
        
        **3. Chat libre:**
        - Posez des questions sans fichier
        - L'IA utilise ses connaissances g√©n√©rales
        
        ### üìÑ Formats support√©s
        - **Documents:** PDF, Word (docx/doc), PowerPoint (pptx), RTF, TXT
        - **Donn√©es:** Excel (xlsx/xls), CSV
        - **Images:** PNG, JPG, JPEG, GIF, BMP, TIFF (utiliser LlaVa)
        
        ### ü§ñ Mod√®les disponibles
        - **Llama 3:** Polyvalent et performant
        - **Mistral:** Rapide et efficace  
        - **CodeLlama:** Sp√©cialis√© en code
        - **Gemma:** √âquilibr√© et pr√©cis
        - **LlaVa:** Analyse les images
        
        ### üí° Conseils
        - Utilisez LlaVa pour analyser les images
        - Les gros fichiers peuvent prendre plus de temps
        - V√©rifiez qu'Ollama est d√©marr√© avant d'utiliser l'application
        """)

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        st.error(f"Erreur critique de l'application: {e}")
        st.info("üí° V√©rifiez que toutes les d√©pendances sont install√©es et qu'Ollama est d√©marr√©")